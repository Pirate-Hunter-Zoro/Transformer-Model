{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkBoCeLagwDv"
      },
      "source": [
        "# CS-5313/7313 Project 4\n",
        "\n",
        "## IMPORTANT - Make a copy of this colab notebook before working on it.\n",
        "\n",
        "In this project, you will be exploring various pre-built language models based on the transformer architecture. Transformer networks are a state of the art approach to langauge and time series modeling that makes use a concept called \"attention\". The first time this design was proposed was in the paper \"Attention is All You Need\" by Vaswani et al. The paper can be read here: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "Here you will be making use of the pre-built transformer pipelines provided by Hugging Face Co. You can reference this link on how to use the package for the given task you are trying to complete: https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh_aZggPmE-C"
      },
      "source": [
        "## Task 1 - Text generation\n",
        "\n",
        "In this task you will be using the \"text-generation\" pipeline to generate text.\n",
        "\n",
        "Use three different sized prompts, 1 word, ~5 words, and ~10 words, to generate sequences of length n+5 words, n+20 words, and 100 words, where n is the number of words in the prompt phrase you provided. Generate 3 sequences for each prompt and output length pair. Since this is qualitative, comment on the relative quality of the text that is generated in your report and include examples. How do the parameters affect the quality of the output. In addition to the report, submit another document containing each of these generated sequences, including what the prompt was."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i3XvfE9hnrom"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All generations saved to: generated_responses.txt\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "from transformers import pipeline\n",
        "\n",
        "# Suppress HF logs\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "# Prompts\n",
        "one_word_prompt = \"Asta\"\n",
        "five_word_prompt = \"Jotaro's stand is called Star\"\n",
        "ten_word_prompt = \"Princess Mononoke is a film by Hayao Miyazaki all about\"\n",
        "prompts = [one_word_prompt, five_word_prompt, ten_word_prompt]\n",
        "\n",
        "# Setup generator\n",
        "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "# Output file\n",
        "output_path = \"generated_responses.txt\"\n",
        "\n",
        "# Generate and save to file\n",
        "with open(output_path, \"w\") as f:\n",
        "    for prompt in prompts:\n",
        "        f.write(f\"Prompt: {prompt}\\n\")\n",
        "        word_count = len(prompt.split())\n",
        "        for offset in [5, 10, 20]:\n",
        "            max_len = (word_count + offset) * 5  # estimate token count - because that's what the max length is talking about\n",
        "            results = generator(\n",
        "                prompt,\n",
        "                max_length=max_len,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True,\n",
        "                temperature=0.9,\n",
        "                top_p=0.95,\n",
        "                pad_token_id=50256\n",
        "            )\n",
        "            for res in results:\n",
        "                continuation = res['generated_text'].strip()\n",
        "                f.write(f\"→ Response (~{max_len} tokens): {continuation}\\n\")\n",
        "        f.write(\"\\n\" + \"-\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"✅ All generations saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### You'll have to run everything from here on out on Google Colab..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tccR2Sk2inO6"
      },
      "source": [
        "## Task 2 - Sentiment Analysis\n",
        "\n",
        "Here you will be using the \"sentiment-analysis\" pipeline to look at the sentiment of Amazon reviews. The database is provided at this link: https://jmcauley.ucsd.edu/data/amazon/ . It is recommended to use one of the smaller databases, such as the Musical Instruments database with 10,261 reviews.\n",
        "\n",
        "Each review has both the text of the review, as well as the reviewer's rating.\n",
        "\n",
        "### Subtask 2.1\n",
        "Perform sentiment analysis on each review, and compare the model's output to the users review to get a sense of the accuracy of the model. The user review score, which is out of a maximum 5 stars, is found in the \"overall\" datafield. For this, assume that 3+ in the \"overall\" datafield is a positive review.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x97twvbOimmk"
      },
      "outputs": [],
      "source": [
        "# Subtask 2.1\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "classifier = pipeline(\"sentiment-analysis\", truncation=True, padding=True, device=device)\n",
        "\n",
        "# Load data\n",
        "reviews = []\n",
        "with open(\"Musical_Instruments.json\", 'r') as f:\n",
        "    for line in f:\n",
        "        # Create list of json objects by the line\n",
        "        reviews.append(json.loads(line))\n",
        "\n",
        "# Filter usable reviews (non-empty text + score present)\n",
        "filtered_reviews = [\n",
        "    r for r in reviews\n",
        "    if r.get(\"reviewText\") is not None and isinstance(r.get(\"overall\"),float)\n",
        "]\n",
        "\n",
        "# Batch size for review analysis\n",
        "batch_size = 16\n",
        "\n",
        "# Run sentiment analysis in batches\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for i in tqdm(range(0, len(filtered_reviews), batch_size)):\n",
        "    batch = filtered_reviews[i:i+batch_size]\n",
        "    # Grab the information for this batch of reviews\n",
        "    texts = [r[\"reviewText\"][:512] for r in batch]  # Truncate text input BECAUSE APPARENLTY THERE IS A TOKEN LIMIT\n",
        "    true_scores = [r[\"overall\"] for r in batch]\n",
        "\n",
        "    try:\n",
        "        # Parallel runtime for the batch\n",
        "        predictions = classifier(texts)\n",
        "        for pred, score in zip(predictions, true_scores):\n",
        "            model_sentiment = pred[\"label\"] == \"POSITIVE\" # True - positive review\n",
        "            # Our criteria is that a score of 3 or higher is a positive review\n",
        "            actual_sentiment = score >= 3\n",
        "            if model_sentiment == actual_sentiment:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Batch at index {i} failed: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Accuracy: {correct / total:.2%} over {total} reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "![Task 2.1](Results%20Task%202.1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subtask 2.2\n",
        "In addition to looking at the accuracy of the model for each review, also compare the percentage of products with more positive reviews than negative reviews to the true percentage.\n",
        "\n",
        "Reminder: Refer to the hugging face link on how to perform sentiment analysis task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of products with more positive reviews: 90.75%\n"
          ]
        }
      ],
      "source": [
        "# Subtask 2.2\n",
        "# First we need to group the data by product ID\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load data\n",
        "reviews_by_product = defaultdict(list)\n",
        "with open(\"Musical_Instruments.json\", 'r') as f:\n",
        "    for line in f:\n",
        "        # Create list of json objects by the line\n",
        "        review = json.loads(line)\n",
        "        product_id = review.get(\"asin\")\n",
        "        if product_id:\n",
        "            reviews_by_product[product_id].append(review)\n",
        "\n",
        "# Now we can analyze the reviews for each product\n",
        "count_positive = 0\n",
        "count_negative = 0\n",
        "for product_id, reviews in reviews_by_product.items():\n",
        "    # Filter out empty reviews\n",
        "    filtered_reviews = [r for r in reviews if r.get(\"reviewText\") and isinstance(r.get(\"overall\"), float)]\n",
        "    if not filtered_reviews:\n",
        "        continue\n",
        "\n",
        "    # Calculate the average score\n",
        "    avg_score = sum(r[\"overall\"] for r in filtered_reviews) / len(filtered_reviews)\n",
        "    if avg_score >= 3:\n",
        "        count_positive += 1\n",
        "    else:\n",
        "        count_negative += 1\n",
        "\n",
        "print(f\"Percentage of products with more positive reviews: {count_positive / (count_positive + count_negative) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO Task 3\n",
        "# Subtask 2.1\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "classifier = pipeline(\"sentiment-analysis\", truncation=True, padding=True, device=device)\n",
        "\n",
        "# Load data\n",
        "reviews = []\n",
        "with open(\"Musical_Instruments.json\", 'r') as f:\n",
        "    for line in f:\n",
        "      try:\n",
        "          reviews.append(json.loads(line))\n",
        "      except json.JSONDecodeError as e:\n",
        "          print(f\"⚠️ Skipping malformed line: {e}\")\n",
        "\n",
        "# Filter usable reviews (non-empty text + score present)\n",
        "filtered_reviews = [\n",
        "    r for r in reviews\n",
        "    if r.get(\"reviewText\") is not None and isinstance(r.get(\"overall\"),float)\n",
        "]\n",
        "\n",
        "# Batch size for review analysis\n",
        "batch_size = 16\n",
        "\n",
        "# Run sentiment analysis in batches\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Create a dictionary to store the positive and negative review counts for each product\n",
        "counts_by_product = defaultdict(lambda: {\"POSITIVE\": 0, \"NEGATIVE\": 0})\n",
        "\n",
        "for i in tqdm(range(0, len(filtered_reviews), batch_size)):\n",
        "    batch = filtered_reviews[i:i+batch_size]\n",
        "    # Grab the information for this batch of reviews\n",
        "    texts = [r[\"reviewText\"][:512] for r in batch]  # Truncate text input BECAUSE APPARENLTY THERE IS A TOKEN LIMIT\n",
        "    product_ids = [r[\"asin\"] for r in batch]\n",
        "\n",
        "    try:\n",
        "        # Parallel runtime for the batch\n",
        "        predictions = classifier(texts)\n",
        "        for pred, id in zip(predictions, product_ids):\n",
        "            counts_by_product[id][pred[\"label\"]] += 1\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Batch at index {i} failed: {e}\")\n",
        "\n",
        "positive_count = sum([1 for v in counts_by_product.values() if v[\"POSITIVE\"] > v[\"NEGATIVE\"]])\n",
        "negative_count = len(counts_by_product) - positive_count\n",
        "print(f\"Percentage of products with more PREDICTED positive reviews: {positive_count / (positive_count + negative_count) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "![Task 2.2](Results%20Task%202.2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mldUBJ7vmDJe"
      },
      "source": [
        "## Task 3 - Masked Language Modeling\n",
        "\n",
        "In this task you will be completing a sentence or phrase that has a missing word. I will prepare three datasets for you so you can perform this task. The three datasets will contain missing verbs in one, missing nouns in another, and missing adjectives in the last.\n",
        "\n",
        "Your task will be to look at the success rate of generating the true missing word in the top 1, top 5, and top 10 generated words for a given sentence. You will then compare the success rate between the three datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DjI2TIBIlPS9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mikeyferguson/Developer/Transformer-Model/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# We need pre-trained tokenizers and prediction models\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    model.to(device)\n",
        "\n",
        "def prepare_input(sentence):\n",
        "    # Necessary because bert-base-uncased expects a [MASK] token instead of __MASKED__\n",
        "    return sentence.replace(\"__MASKED__\", \"[MASK]\")\n",
        "\n",
        "def predict_masked_word(sentence, top_k=10):\n",
        "    masked_sentence = prepare_input(sentence)\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "    # Check for presence of [MASK] token\n",
        "    mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "    if len(mask_token_indices) == 0:\n",
        "        print(f\"⚠️ No [MASK] token found in: {sentence}\")\n",
        "        return []\n",
        "\n",
        "    mask_index = mask_token_indices[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    mask_logits = logits[0, mask_index, :]\n",
        "    topk_ids = torch.topk(mask_logits, top_k, dim=0).indices.tolist()\n",
        "\n",
        "    return [tokenizer.decode([idx]).strip() for idx in topk_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_mlm(dataset):\n",
        "    top_1 = 0\n",
        "    top_5 = 0\n",
        "    top_10 = 0\n",
        "    total = len(dataset)\n",
        "\n",
        "    for example in dataset:\n",
        "        # Each example is a sentence with a __MASK__ token (turned into [MASK] in the above function)\n",
        "        text = example[\"text\"]\n",
        "        true_word = example[\"label\"]\n",
        "        predictions = predict_masked_word(text, top_k=10)\n",
        "\n",
        "        if true_word == predictions[0]:\n",
        "            top_1 += 1\n",
        "        if true_word in predictions[:5]:\n",
        "            top_5 += 1\n",
        "        if true_word in predictions[:10]:\n",
        "            top_10 += 1\n",
        "\n",
        "    return {\n",
        "        \"top_1_acc\": top_1 / total,\n",
        "        \"top_5_acc\": top_5 / total,\n",
        "        \"top_10_acc\": top_10 / total\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get our hands on the verbs, nouns, and adjectives data\n",
        "import csv\n",
        "\n",
        "def load_words(file_name):\n",
        "    data = []\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, quotechar='\"', delimiter=',', skipinitialspace=True)\n",
        "        for row in reader:\n",
        "            if len(row) != 2:\n",
        "                print(f\"⚠️ Bad row: {row}\")\n",
        "                continue\n",
        "            sentence, label = row\n",
        "            data.append({\"text\": sentence, \"label\": label.strip()})\n",
        "    return data\n",
        "\n",
        "verb_data = load_words(\"masked_verbs.csv\")\n",
        "noun_data = load_words(\"masked_nouns.csv\")\n",
        "adj_data = load_words(\"masked_adjs.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "VERBS dataset:\n",
            "  top_1_acc: 26.60%\n",
            "  top_5_acc: 52.20%\n",
            "  top_10_acc: 63.40%\n",
            "\n",
            "NOUNS dataset:\n",
            "  top_1_acc: 18.60%\n",
            "  top_5_acc: 32.40%\n",
            "  top_10_acc: 37.80%\n",
            "\n",
            "ADJECTIVES dataset:\n",
            "  top_1_acc: 27.20%\n",
            "  top_5_acc: 48.00%\n",
            "  top_10_acc: 55.20%\n"
          ]
        }
      ],
      "source": [
        "# Run on all three datasets\n",
        "results = {}\n",
        "for category, dataset in [(\"verbs\", verb_data), (\"nouns\", noun_data), (\"adjectives\", adj_data)]:\n",
        "    results[category] = evaluate_mlm(dataset)\n",
        "\n",
        "# Optional: Pretty print\n",
        "for category, scores in results.items():\n",
        "    print(f\"\\n{category.upper()} dataset:\")\n",
        "    for k, v in scores.items():\n",
        "        print(f\"  {k}: {v:.2%}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
