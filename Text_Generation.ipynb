{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkBoCeLagwDv"
      },
      "source": [
        "# CS-5313/7313 Project 4\n",
        "\n",
        "## IMPORTANT - Make a copy of this colab notebook before working on it.\n",
        "\n",
        "In this project, you will be exploring various pre-built language models based on the transformer architecture. Transformer networks are a state of the art approach to langauge and time series modeling that makes use a concept called \"attention\". The first time this design was proposed was in the paper \"Attention is All You Need\" by Vaswani et al. The paper can be read here: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "Here you will be making use of the pre-built transformer pipelines provided by Hugging Face Co. You can reference this link on how to use the package for the given task you are trying to complete: https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh_aZggPmE-C"
      },
      "source": [
        "## Task 1 - Text generation\n",
        "\n",
        "In this task you will be using the \"text-generation\" pipeline to generate text.\n",
        "\n",
        "Use three different sized prompts, 1 word, ~5 words, and ~10 words, to generate sequences of length n+5 words, n+20 words, and 100 words, where n is the number of words in the prompt phrase you provided. Generate 3 sequences for each prompt and output length pair. Since this is qualitative, comment on the relative quality of the text that is generated in your report and include examples. How do the parameters affect the quality of the output. In addition to the report, submit another document containing each of these generated sequences, including what the prompt was."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i3XvfE9hnrom"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All generations saved to: generated_responses.txt\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "from transformers import pipeline\n",
        "\n",
        "# Suppress HF logs\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "# Prompts\n",
        "one_word_prompt = \"Asta\"\n",
        "five_word_prompt = \"Jotaro's stand is called Star\"\n",
        "ten_word_prompt = \"Princess Mononoke is a film by Hayao Miyazaki all about\"\n",
        "prompts = [one_word_prompt, five_word_prompt, ten_word_prompt]\n",
        "\n",
        "# Setup generator\n",
        "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "# Output file\n",
        "output_path = \"generated_responses.txt\"\n",
        "\n",
        "# Generate and save to file\n",
        "with open(output_path, \"w\") as f:\n",
        "    for prompt in prompts:\n",
        "        f.write(f\"Prompt: {prompt}\\n\")\n",
        "        word_count = len(prompt.split())\n",
        "        for offset in [5, 10, 20]:\n",
        "            max_len = (word_count + offset) * 5  # estimate token count - because that's what the max length is talking about\n",
        "            results = generator(\n",
        "                prompt,\n",
        "                max_length=max_len,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True,\n",
        "                temperature=0.9,\n",
        "                top_p=0.95,\n",
        "                pad_token_id=50256\n",
        "            )\n",
        "            for res in results:\n",
        "                continuation = res['generated_text'].strip()\n",
        "                f.write(f\"→ Response (~{max_len} tokens): {continuation}\\n\")\n",
        "        f.write(\"\\n\" + \"-\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"✅ All generations saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### You'll have to run everything from here on out on Google Colab..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tccR2Sk2inO6"
      },
      "source": [
        "## Task 2 - Sentiment Analysis\n",
        "\n",
        "Here you will be using the \"sentiment-analysis\" pipeline to look at the sentiment of Amazon reviews. The database is provided at this link: https://jmcauley.ucsd.edu/data/amazon/ . It is recommended to use one of the smaller databases, such as the Musical Instruments database with 10,261 reviews.\n",
        "\n",
        "Each review has both the text of the review, as well as the reviewer's rating.\n",
        "\n",
        "### Subtask 2.1\n",
        "Perform sentiment analysis on each review, and compare the model's output to the users review to get a sense of the accuracy of the model. The user review score, which is out of a maximum 5 stars, is found in the \"overall\" datafield. For this, assume that 3+ in the \"overall\" datafield is a positive review.\n",
        "\n",
        "### Subtask 2.2\n",
        "In addition to looking at the accuracy of the model for each review, also compare the percentage of products with more positive reviews than negative reviews to the true percentage.\n",
        "\n",
        "Reminder: Refer to the hugging face link on how to perform sentiment analysis task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x97twvbOimmk"
      },
      "outputs": [],
      "source": [
        "# Subtask 2.1\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "classifier = pipeline(\"sentiment-analysis\", truncation=True, padding=True, device=device)\n",
        "\n",
        "# Load data\n",
        "reviews = []\n",
        "with open(\"Musical_Instruments.json\", 'r') as f:\n",
        "    for line in f:\n",
        "        # Create list of json objects by the line\n",
        "        reviews.append(json.loads(line))\n",
        "\n",
        "# Filter usable reviews (non-empty text + score present)\n",
        "filtered_reviews = [\n",
        "    r for r in reviews\n",
        "    if r.get(\"reviewText\") is not None and isinstance(r.get(\"overall\"),float)\n",
        "]\n",
        "\n",
        "# Batch size for review analysis\n",
        "batch_size = 16\n",
        "\n",
        "# Run sentiment analysis in batches\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for i in tqdm(range(0, len(filtered_reviews), batch_size)):\n",
        "    batch = filtered_reviews[i:i+batch_size]\n",
        "    # Grab the information for this batch of reviews\n",
        "    texts = [r[\"reviewText\"][:512] for r in batch]  # Truncate text input BECAUSE APPARENLTY THERE IS A TOKEN LIMIT\n",
        "    true_scores = [r[\"overall\"] for r in batch]\n",
        "\n",
        "    try:\n",
        "        # Parallel runtime for the batch\n",
        "        predictions = classifier(texts)\n",
        "        for pred, score in zip(predictions, true_scores):\n",
        "            model_sentiment = pred[\"label\"] == \"POSITIVE\" # True - positive review\n",
        "            # Our criteria is that a score of 3 or higher is a positive review\n",
        "            actual_sentiment = score >= 3\n",
        "            if model_sentiment == actual_sentiment:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Batch at index {i} failed: {e}\")\n",
        "\n",
        "with open(\"sentiment_analysis_results.txt\", \"w\") as file:\n",
        "    file.write(f\"\\n✅ Accuracy: {correct / total:.2%} over {total} reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "![Task 2.1](Results%20Task%202.1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mldUBJ7vmDJe"
      },
      "source": [
        "## Task 3 - Masked Language Modeling\n",
        "\n",
        "In this task you will be completing a sentence or phrase that has a missing word. I will prepare three datasets for you so you can perform this task. The three datasets will contain missing verbs in one, missing nouns in another, and missing adjectives in the last.\n",
        "\n",
        "Your task will be to look at the success rate of generating the true missing word in the top 1, top 5, and top 10 generated words for a given sentence. You will then compare the success rate between the three datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjI2TIBIlPS9"
      },
      "outputs": [],
      "source": [
        "# TODO Task 3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
